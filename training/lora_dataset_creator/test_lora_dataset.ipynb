{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<s1> drinking a beer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\n",
    "    \"cuda\"\n",
    ")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "image = pipe(prompt, num_inference_steps=25, guidance_scale=9.5).images[0]\n",
    "\n",
    "image  # nice. diffusers are cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is two way to LORA this model. You can 1. monkey-patch it, or 2. update the weight inplaced.\n",
    "\n",
    "Monkey-patching is essentially replacing the linear layer with a lora-linear layer, which is the following\n",
    "\n",
    "$$\n",
    "x_2 = Wx_1 + A B^T x_1\n",
    "$$\n",
    "\n",
    "On the other hand, weight updating is literally replacing the original weight with the LORA weight. This is the following\n",
    "\n",
    "$$\n",
    "W' = W + A B^T\n",
    "$$\n",
    "\n",
    "You might find this weird. Just having the weight updated is the logical option. Why even monkey-patch when you can add the weights? Well, by keeping the LORA weights we can perform _weight mixing_ dynamically. We can't do this if we just update the weight, because the weight is fixed. This is the reason why we have two options. You can adjust the weight with `tune_lora_scale` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_diffusion import monkeypatch_lora, tune_lora_scale, patch_pipe\n",
    "\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"/Projects/Personal/leap-sd/training/lora_dataset_creator/lora_dataset/dragon/models/step_1000.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "\n",
    "tune_lora_scale(pipe.unet, 1.00)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=9).images[0]\n",
    "display(image)\n",
    "\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"/Projects/Personal/leap-sd/training/lora_dataset_creator/lora_dataset/dragon/models/step_inv_1000.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "\n",
    "tune_lora_scale(pipe.unet, 1.00)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=9).images[0]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_lora_scale(pipe.unet, 0.3)\n",
    "tune_lora_scale(pipe.text_encoder, 0.3)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=9).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice. Let's try another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\n",
    "    \"cuda\"\n",
    ")\n",
    "\n",
    "prompt = \"superman, style of <s1><s2>\"\n",
    "torch.manual_seed(1)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=4).images[0]\n",
    "\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"../example_loras/lora_popart.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "torch.manual_seed(1)\n",
    "tune_lora_scale(pipe.unet, 1.0)\n",
    "tune_lora_scale(pipe.text_encoder, 1.0)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=4).images[0]\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good pop-art style, but we might get a better result with lower $\\alpha$ for both text encoder and unet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "tune_lora_scale(pipe.unet, 0.5)\n",
    "tune_lora_scale(pipe.text_encoder, 0.5)\n",
    "\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=4.5).images[0]\n",
    "image.save(\"../contents/pop_art.jpg\")\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix : To make stuff on the readme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"baby lion in style of <s1><s2>\"\n",
    "\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"../example_loras/lora_disney.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "torch.manual_seed(6)\n",
    "tune_lora_scale(pipe.unet, 0.5)\n",
    "tune_lora_scale(pipe.text_encoder, 0.5)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=5).images[0]\n",
    "image.save(\"../contents/disney_lora.jpg\")\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"../example_loras/lora_krk.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "\n",
    "example_prompts = [\n",
    "    \"painting of <TOK>, a starry night, style of vincent van gogh\",\n",
    "    \"portrait of <TOK> by mario testino 1950, 1950s style, hair tied in a bun, taken in 1950, detailed face of <TOK>, sony a7r\",\n",
    "    \"photof of <TOK>, 50mm, sharp, muscular, detailed realistic face, hyper realistic, perfect face, intricate, natural light, <TOK> underwater photoshoot,collarbones, skin indentation, Alphonse Mucha, Greg Rutkowski\",\n",
    "    \"a photo of <TOK> in advanced organic armor, biological filigree, detailed symmetric face, flowing hair, neon details, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, octane, art by Krenz Cushart , Artem Demura, Alphonse Mucha, digital cgi art 8K HDR by Yuanyuan Wang photorealistic\",\n",
    "    \"a photo of <TOK> on the beach, small waves, detailed symmetric face, beautiful composition\",\n",
    "    \"a photo of <TOK> rainbow background, wlop, dan mumford, artgerm, liam brazier, peter mohrbacher, jia zhangke, 8 k, raw, featured in artstation, octane render, cinematic, elegant, intricate, 8 k\",\n",
    "    \"photo of Summoner <TOK> with a cute water elemental, fantasy illustration, detailed face, intricate, elegant, highly detailed, digital painting, artstation, concept art, wallpaper, smooth, sharp focus, illustration, art by artgerm and greg rutkowski\",\n",
    "    \"<TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\",\n",
    "    \"a pencil sketch of <TOK>\",\n",
    "    \"a minecraft render of <TOK>\",\n",
    "    \"young woman <TOK>, eden, intense eyes, tears running down, crying, vaporwave aesthetic, synthwave, colorful, psychedelic, crown, long gown, flowers, bees, butterflies, ribbons, ornate, intricate, digital painting, artstation, concept art, smooth, sharp focus, illustration of <wday>, art by artgerm and greg rutkowski and alphonse mucha\",\n",
    "    \"<TOK> in a construction outfit\",\n",
    "]\n",
    "\n",
    "outs = []\n",
    "tune_lora_scale(pipe.unet, 0.7)\n",
    "tune_lora_scale(pipe.text_encoder, 0.7)\n",
    "for idx, prompt in enumerate(example_prompts):\n",
    "    prompt = prompt.replace(\"<TOK>\", \"<s1><s2>\")\n",
    "    torch.manual_seed(idx)\n",
    "    image = pipe(prompt, num_inference_steps=50, guidance_scale=5).images[0]\n",
    "    outs.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_diffusion import image_grid\n",
    "\n",
    "imgs = image_grid(outs, 3, 4)\n",
    "imgs.save(\"../contents/lora_pti_example.jpg\")\n",
    "imgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "952e1bebe1b278d85469a034aefc1854b777c1b518feedf8249123f6f86cec05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
