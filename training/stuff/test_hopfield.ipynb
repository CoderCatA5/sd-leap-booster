{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a132295-b0ac-479d-a875-ba687e14db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from leap_sd import LEAPBuffer\n",
    "from hflayers import HopfieldLayer\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86f7ba6-8be6-4022-bc27-0364a170b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in = 128 * 4\n",
    "size_out = 509248\n",
    "amount_data = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efc05a3-702b-4c88-8a81-213b6843ef74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datamodule(batch_size: int):\n",
    "    class FakeDataset(Dataset):\n",
    "        def __init__(self, amount):\n",
    "            self.amount = amount\n",
    "            self.y = torch.zeros(amount, size_out).uniform_(0, 1)\n",
    "            self.x = self.y[:, :size_in]\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.x[index, ...], self.y[index, ...]\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.amount\n",
    "\n",
    "    class ImageWeights(pl.LightningDataModule):\n",
    "        def __init__(self, batch_size: int):\n",
    "            super().__init__()\n",
    "            self.num_workers = 16\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        def prepare_data(self):\n",
    "            pass\n",
    "\n",
    "        def setup(self, stage):\n",
    "            pass\n",
    "            \n",
    "        def train_dataloader(self):\n",
    "            dataset = FakeDataset(amount_data)\n",
    "            return DataLoader(dataset, num_workers = self.num_workers, batch_size = self.batch_size)\n",
    "\n",
    "        def teardown(self, stage):\n",
    "            # clean up after fit or test\n",
    "            # called on every process in DDP\n",
    "            pass\n",
    "    \n",
    "    dm = ImageWeights(batch_size = batch_size)\n",
    "    \n",
    "    return dm\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_lookup_weights(hopfield, loader):\n",
    "    Z = None\n",
    "    for x, _ in loader:\n",
    "        if Z is None:\n",
    "            Z = x\n",
    "        else:\n",
    "            Z = torch.cat((Z, x), dim=0)\n",
    "    Z = Z.unsqueeze(0)\n",
    "    print(\"set_lookup_weights > X\", Z.shape)\n",
    "    hopfield.lookup_weights[:] = Z\n",
    "\n",
    "class TestModel(pl.LightningModule):\n",
    "    def __init__(self, size_in: int, size_out: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "        self.buf = HopfieldLayer(\n",
    "            input_size=size_in,\n",
    "            output_size=size_out,\n",
    "            hidden_size=20,\n",
    "            num_heads=16,\n",
    "            pattern_projection_as_connected=False,\n",
    "            quantity=amount_data,\n",
    "            scaling=8.0,\n",
    "            dropout=0.5,\n",
    "            lookup_weights_as_separated=True,\n",
    "            lookup_targets_as_trainable=False\n",
    "        )\n",
    "        # self.buf = LEAPBuffer(size_in, size_out, 2048, 5, 0.01)\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # x = x + torch.zeros_like(x).uniform_(0, 0.001)\n",
    "        result = self.buf(x.unsqueeze(1)).squeeze(1)\n",
    "        loss = self.criterion(result, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        cur_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.log(\"lr\", cur_lr, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5),\n",
    "            \"monitor\": \"train_loss\",\n",
    "            \"interval\": \"epoch\"\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "def train():\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    model = TestModel(size_in, size_out, 1e-3)\n",
    "    dm = get_datamodule(10)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    set_lookup_weights(model.buf, dm.train_dataloader())\n",
    "    trainer = pl.Trainer(auto_lr_find=True, devices=1, accelerator=\"gpu\", callbacks = [lr_monitor], log_every_n_steps=2, max_epochs=100)\n",
    "    # trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger = WandbLogger(project=\"LEAP_Lora_BufferTest\"), callbacks = [lr_monitor], log_every_n_steps=2, max_epochs=1000)\n",
    "    # trainer.tune(model, dm)\n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f03664-cc5d-45e6-8e50-b887d2b24526",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4a305e73d9ba6c54\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4a305e73d9ba6c54\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3d450-1d72-4798-89e1-af491cc9f42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888ea07-39ff-4d6e-9a53-c8297c5e4e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
