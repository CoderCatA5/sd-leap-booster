{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a132295-b0ac-479d-a875-ba687e14db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from leap_sd import LEAPBuffer\n",
    "from hflayers import HopfieldLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f7ba6-8be6-4022-bc27-0364a170b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in = 128\n",
    "size_out = 500000\n",
    "amount_data = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc05a3-702b-4c88-8a81-213b6843ef74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datamodule(batch_size: int):\n",
    "    class FakeDataset(Dataset):\n",
    "        def __init__(self, amount):\n",
    "            self.amount = amount\n",
    "            self.x = torch.zeros(amount, size_in).uniform_(0, 1)\n",
    "            self.y = torch.zeros(amount, size_out).uniform_(0, 1)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            return self.x[index, ...], self.y[index, ...]\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.amount\n",
    "\n",
    "    class ImageWeights(pl.LightningDataModule):\n",
    "        def __init__(self, batch_size: int):\n",
    "            super().__init__()\n",
    "            self.num_workers = 16\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        def prepare_data(self):\n",
    "            pass\n",
    "\n",
    "        def setup(self, stage):\n",
    "            pass\n",
    "            \n",
    "        def train_dataloader(self):\n",
    "            dataset = FakeDataset(amount_data)\n",
    "            return DataLoader(dataset, num_workers = self.num_workers, batch_size = self.batch_size)\n",
    "\n",
    "        def teardown(self, stage):\n",
    "            # clean up after fit or test\n",
    "            # called on every process in DDP\n",
    "            pass\n",
    "    \n",
    "    dm = ImageWeights(batch_size = batch_size)\n",
    "    \n",
    "    return dm\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_lookup_weights(hopfield, loader):\n",
    "    Z = None\n",
    "    for x, _ in loader:\n",
    "        if Z is None:\n",
    "            Z = x\n",
    "        else:\n",
    "            Z = torch.cat((Z, x), dim=0)\n",
    "    Z = Z.unsqueeze(0)\n",
    "    print(\"set_lookup_weights > X\", Z.shape)\n",
    "    hopfield.lookup_weights[:] = Z\n",
    "\n",
    "class TestModel(pl.LightningModule):\n",
    "    def __init__(self, size_in: int, size_out: int, hidden_size: int, learning_rate: float):\n",
    "        super().__init__()\n",
    "        self.buf = HopfieldLayer(\n",
    "            input_size=size_in,\n",
    "            output_size=size_out,\n",
    "            hidden_size=hidden_size,\n",
    "            num_heads=4,\n",
    "            quantity=amount_data,\n",
    "            scaling=8.0,\n",
    "            dropout=0,\n",
    "            lookup_weights_as_separated=True,\n",
    "            lookup_targets_as_trainable=False,\n",
    "            # do not pre-process layer input\n",
    "            # normalize_stored_pattern=False,\n",
    "            # normalize_stored_pattern_affine=False,\n",
    "            # normalize_state_pattern=False,\n",
    "            # normalize_state_pattern_affine=False,\n",
    "            # normalize_pattern_projection=False,\n",
    "            # normalize_pattern_projection_affine=False,\n",
    "        )\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        result = self.buf(x.unsqueeze(1)).squeeze(1)\n",
    "        loss = self.criterion(result, y)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        cur_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.log(\"lr\", cur_lr, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5),\n",
    "            \"monitor\": \"train_loss\",\n",
    "            \"interval\": \"epoch\"\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "def train():\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "    hidden_size = 20\n",
    "    model = TestModel(size_in, size_out, hidden_size, 1e-3)\n",
    "    dm = get_datamodule(10)\n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    from pytorch_lightning.loggers import WandbLogger\n",
    "    set_lookup_weights(model.buf, dm.train_dataloader())\n",
    "    trainer = pl.Trainer(auto_lr_find=True, devices=1, accelerator=\"gpu\", callbacks = [lr_monitor], log_every_n_steps=2, max_epochs=100)\n",
    "    # trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger = WandbLogger(project=\"LEAP_Lora_BufferTest\"), callbacks = [lr_monitor], log_every_n_steps=2, max_epochs=1000)\n",
    "    # trainer.tune(model, dm)\n",
    "    trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd3d450-1d72-4798-89e1-af491cc9f42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d748f-d267-4376-b4f6-90a80d1aa61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
